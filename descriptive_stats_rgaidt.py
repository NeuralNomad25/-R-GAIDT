# -*- coding: utf-8 -*-
"""Descriptive_Stats_RGAIDT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WnZ_8N7r-1Szt9MzKtWMRE38UM5-Mc0O
"""

import pandas as pd

data_pre=pd.read_csv('PreOriginalData_sample.csv')
data_post=pd.read_csv('Post-interventionOrigninalSample.csv')

data_pre.rename(columns={data_pre.columns[1]: 'Problem Solving Skills'}, inplace=True)
data_pre.rename(columns={data_pre.columns[2]: 'Critical Thinking'}, inplace=True)
data_pre.rename(columns={data_pre.columns[4]: 'Creativity'}, inplace=True)
data_post.rename(columns={data_post.columns[11]: 'Problem Solving Skills'}, inplace=True)
data_post.rename(columns={data_post.columns[6]: 'Critical Thinking'}, inplace=True)
data_post.rename(columns={data_post.columns[4]: 'Creativity'}, inplace=True)

problem_solving_mapping_post = {
    'Significantly Improved After AI Use': 5,
    'Slightly Improved After AI Use': 4,
    'No Change': 3,
    'Slightly Worse After AI Use': 2,
    'Significantly Worse After AI Use': 1  # Added for completeness
}

problem_solving_mapping_pre = {
    'Confident': 4,
    'Slightly Confident': 3,
    'Neutral': 2,
    'Slightly Worse': 1,  # Assuming possible other responses
    'Significantly Worse': 1  # Added for consistency
}

creativity_mapping_post = {
    'Often': 5,
    'Sometimes': 3,
    'Rarely': 1
}

creativity_mapping_pre = {
    'Comfortable': 5,
    'Slightly Comfortable': 3,
    'Neutral': 1
}

creativity_mapping_post = {
    'Often': 5,
    'Sometimes': 3,
    'Rarely': 1
}

creativity_mapping_pre = {
    'Comfortable': 5,
    'Slightly Comfortable': 3,
    'Neutral': 1
}

critical_thinking_mapping_post = {
    'Significantly Better After AI Use': 5,
    'Slightly Better After AI Use': 4,
    'No Change': 3,
    'Slightly Worse After AI Use': 2,
    'Significantly Worse After AI Use': 1
}

critical_thinking_mapping_pre = {
    'Very Significantly': 5,
    'Significantly': 4,
    'Moderately': 3,
    'Slightly': 2,
    'Not at All': 1
}



# Apply the updated mappings for data_post
data_post['Problem Solving Skills'] = data_post['Problem Solving Skills'].map(problem_solving_mapping_post)
data_post['Creativity'] = data_post['Creativity'].map(creativity_mapping_post)
data_post['Critical Thinking'] = data_post['Critical Thinking'].map(critical_thinking_mapping_post)

# Apply the updated mappings for data_pre
data_pre['Problem Solving Skills'] = data_pre['Problem Solving Skills'].map(problem_solving_mapping_pre)
data_pre['Creativity'] = data_pre['Creativity'].map(creativity_mapping_pre)
data_pre['Critical Thinking'] = data_pre['Critical Thinking'].map(critical_thinking_mapping_pre)

# Select only the projected columns for data_post and save to CSV
data_post[['Problem Solving Skills', 'Creativity', 'Critical Thinking']].to_csv('data_post_processed.csv', index=False)

# Select only the projected columns for data_pre and save to CSV
data_pre[['Problem Solving Skills', 'Creativity', 'Critical Thinking']].to_csv('data_pre_processed.csv', index=False)

# Confirm the files have been saved
print("Projected columns from data_post have been saved to 'data_post_processed.csv'.")
print("Projected columns from data_pre have been saved to 'data_pre_processed.csv'.")


# Confirm the files have been saved
print("Data post has been saved to 'data_post_processed.csv'.")
print("Data pre has been saved to 'data_pre_processed.csv'.")

import pandas as pd

# Example of your updated data_post and data_pre DataFrames
# Assuming data_post and data_pre already have their values mapped as in the previous steps

# Calculate mean, median, and mode for data_post
data_post_mean = data_post[['Problem Solving Skills', 'Creativity', 'Critical Thinking']].mean()
data_post_median = data_post[['Problem Solving Skills', 'Creativity', 'Critical Thinking']].median()
data_post_mode = data_post[['Problem Solving Skills', 'Creativity', 'Critical Thinking']].mode().iloc[0]

# Calculate mean, median, and mode for data_pre
data_pre_mean = data_pre[['Problem Solving Skills', 'Creativity', 'Critical Thinking']].mean()
data_pre_median = data_pre[['Problem Solving Skills', 'Creativity', 'Critical Thinking']].median()
data_pre_mode = data_pre[['Problem Solving Skills', 'Creativity', 'Critical Thinking']].mode().iloc[0]

# Display the results for data_post
print("Data Post - Mean:")
print(data_post_mean)
print("\nData Post - Median:")
print(data_post_median)
print("\nData Post - Mode:")
print(data_post_mode)

# Display the results for data_pre
print("\nData Pre - Mean:")
print(data_pre_mean)
print("\nData Pre - Median:")
print(data_pre_median)
print("\nData Pre - Mode:")
print(data_pre_mode)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming data_pre and data_post DataFrames already exist and have numeric values

# Set Seaborn aesthetics
sns.set(style="whitegrid")

# List of skills/metrics to analyze
metrics = ['Problem Solving Skills', 'Creativity', 'Critical Thinking']

# Create frequency distribution plots for data_pre
plt.figure(figsize=(18, 6))
for i, metric in enumerate(metrics, 1):
    plt.subplot(1, 3, i)

    # Plot histogram with KDE
    sns.histplot(data_pre[metric], kde=True, color='skyblue', bins=20, edgecolor='black')

    # Calculate mean, median, and mode
    mean_value = data_pre[metric].mean()
    median_value = data_pre[metric].median()
    mode_value = data_pre[metric].mode()[0]  # Mode can have multiple values; take the first one

    # Add vertical lines for mean, median, and mode
    plt.axvline(mean_value, color='red', linestyle='--', label=f'Mean: {mean_value:.2f}')
    plt.axvline(median_value, color='green', linestyle='-', label=f'Median: {median_value:.2f}')
    plt.axvline(mode_value, color='blue', linestyle='-.', label=f'Mode: {mode_value:.2f}')

    # Labeling
    plt.xlabel(f'{metric} Score', fontsize=14)
    plt.ylabel('Frequency', fontsize=14)
    plt.title(f'Pre Intervention - {metric}', fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.gca().grid(False)  # Remove the grid

    # Add legend
    plt.legend(fontsize=12)

plt.tight_layout()
plt.savefig("pre-intervention_stats_with_mean_median_mode.png", dpi=300, bbox_inches='tight')
plt.show()

# Create frequency distribution plots for data_post
plt.figure(figsize=(18, 6))
for i, metric in enumerate(metrics, 1):
    plt.subplot(1, 3, i)

    # Plot histogram with KDE
    sns.histplot(data_post[metric], kde=True, color='lightcoral', bins=20, edgecolor='black')

    # Calculate mean, median, and mode
    mean_value = data_post[metric].mean()
    median_value = data_post[metric].median()
    mode_value = data_post[metric].mode()[0]  # Mode can have multiple values; take the first one

    # Add vertical lines for mean, median, and mode
    plt.axvline(mean_value, color='red', linestyle='--', label=f'Mean: {mean_value:.2f}')
    plt.axvline(median_value, color='green', linestyle='-', label=f'Median: {median_value:.2f}')
    plt.axvline(mode_value, color='blue', linestyle='-.', label=f'Mode: {mode_value:.2f}')

    # Labeling
    plt.xlabel(f'{metric} Score', fontsize=14)
    plt.ylabel('Frequency', fontsize=14)
    plt.title(f'Post Intervention - {metric}', fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.gca().grid(False)  # Remove the grid

    # Add legend
    plt.legend(fontsize=12)

plt.tight_layout()
plt.savefig("post-intervention_stats_with_mean_median_mode.png", dpi=300, bbox_inches='tight')
plt.show()

"""## GAN-agumentation on the data"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, BatchNormalization
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

# Function to preprocess and normalize data
def preprocess_data(file_path):
    data = pd.read_csv(file_path)
    scaler = MinMaxScaler()
    data_normalized = scaler.fit_transform(data)
    return data, data_normalized, scaler

# Simplified Generator
def build_generator(latent_dim, data_dim):
    model = Sequential([
        Dense(128, input_dim=latent_dim),
        LeakyReLU(alpha=0.2),
        Dense(64),
        LeakyReLU(alpha=0.2),
        Dense(data_dim, activation='sigmoid')
    ])
    return model

# Simplified Discriminator
def build_discriminator(data_dim):
    model = Sequential([
        Dense(64, input_dim=data_dim),
        LeakyReLU(alpha=0.2),
        Dropout(0.3),
        Dense(32),
        LeakyReLU(alpha=0.2),
        Dense(1, activation='sigmoid')
    ])
    return model

# Train the GAN
def train_gan(generator, discriminator, gan, data, latent_dim, epochs=100, batch_size=64):
    d_losses, g_losses = [], []
    for epoch in range(epochs):
        # Train Discriminator
        idx = np.random.randint(0, data.shape[0], batch_size)
        real_samples = data[idx]
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        fake_samples = generator.predict(noise)

        real_labels = np.ones((batch_size, 1)) * 0.9  # Label smoothing
        fake_labels = np.zeros((batch_size, 1))

        d_loss_real = discriminator.train_on_batch(real_samples, real_labels)
        d_loss_fake = discriminator.train_on_batch(fake_samples, fake_labels)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train Generator
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        valid_labels = np.ones((batch_size, 1))
        g_loss = gan.train_on_batch(noise, valid_labels)

        d_losses.append(d_loss[0])
        g_losses.append(g_loss)

        #if epoch % 50 == 0:
           # print(f"{epoch} [D loss: {d_loss[0]:.4f}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]")

    return d_losses, g_losses

# Generate Data
def generate_data(generator, scaler, num_samples, latent_dim):
    noise = np.random.normal(0, 1, (num_samples, latent_dim))
    generated_data = generator.predict(noise)
    return scaler.inverse_transform(generated_data)

# File paths
pre_file_path = 'data_pre_processed.csv'
post_file_path = 'data_post_processed.csv'

# Preprocess pre-intervention data
pre_data, pre_data_normalized, pre_scaler = preprocess_data(pre_file_path)
pre_dim = pre_data_normalized.shape[1]
latent_dim = 100

# Train GAN for pre-intervention data
generator_pre = build_generator(latent_dim, pre_dim)
discriminator_pre = build_discriminator(pre_dim)
discriminator_pre.compile(optimizer=tf.keras.optimizers.Adam(0.0002), loss='binary_crossentropy', metrics=['accuracy'])

gan_input_pre = tf.keras.Input(shape=(latent_dim,))
gan_output_pre = discriminator_pre(generator_pre(gan_input_pre))
gan_pre = tf.keras.Model(gan_input_pre, gan_output_pre)
gan_pre.compile(optimizer=tf.keras.optimizers.Adam(0.0002), loss='binary_crossentropy')

d_losses_pre, g_losses_pre = train_gan(generator_pre, discriminator_pre, gan_pre, pre_data_normalized, latent_dim, epochs=500)

# Generate and save augmented pre-intervention data
pre_augmented_data = generate_data(generator_pre, pre_scaler, num_samples=100, latent_dim=latent_dim)
pd.DataFrame(pre_augmented_data, columns=pre_data.columns).to_csv('pre_augmented_intervention.csv', index=False)

# Preprocess post-intervention data
post_data, post_data_normalized, post_scaler = preprocess_data(post_file_path)
post_dim = post_data_normalized.shape[1]

# Train GAN for post-intervention data
generator_post = build_generator(latent_dim, post_dim)
discriminator_post = build_discriminator(post_dim)
discriminator_post.compile(optimizer=tf.keras.optimizers.Adam(0.0002), loss='binary_crossentropy', metrics=['accuracy'])

gan_input_post = tf.keras.Input(shape=(latent_dim,))
gan_output_post = discriminator_post(generator_post(gan_input_post))
gan_post = tf.keras.Model(gan_input_post, gan_output_post)
gan_post.compile(optimizer=tf.keras.optimizers.Adam(0.0002), loss='binary_crossentropy')

d_losses_post, g_losses_post = train_gan(generator_post, discriminator_post, gan_post, post_data_normalized, latent_dim, epochs=500)

# Generate and save augmented post-intervention data
post_augmented_data = generate_data(generator_post, post_scaler, num_samples=100, latent_dim=latent_dim)
pd.DataFrame(post_augmented_data, columns=post_data.columns).to_csv('post_augmented_intervention.csv', index=False)

# Plotting Losses for pre-intervention
plt.figure(figsize=(10, 5))
plt.plot(d_losses_pre, label='Pre Discriminator Loss')
plt.plot(g_losses_pre, label='Pre Generator Loss')
plt.title('GAN Training Losses - Pre Intervention')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.savefig('pre_intervention_training_losses.png', dpi=300)
plt.show()

# Plotting Losses for post-intervention
plt.figure(figsize=(10, 5))
plt.plot(d_losses_post, label='Post Discriminator Loss')
plt.plot(g_losses_post, label='Post Generator Loss')
plt.title('GAN Training Losses - Post Intervention')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.savefig('post_intervention_training_losses.png', dpi=300)
plt.show()

data_pre=pd.read_csv('/content/pre_augmented_intervention.csv')
data_post=pd.read_csv('/content/post_augmented_intervention.csv')

import pandas as pd

# Example of your updated data_post and data_pre DataFrames
# Assuming data_post and data_pre already have their values mapped as in the previous steps

# Calculate mean, median, and mode for data_post
data_post_mean = data_post[['Problem Solving Skills', 'Creativity', 'Critical Thinking']].mean()
data_post_median = data_post[['Problem Solving Skills', 'Creativity', 'Critical Thinking']].median()
data_post_mode = data_post[['Problem Solving Skills', 'Creativity', 'Critical Thinking']].mode().iloc[0]

# Calculate mean, median, and mode for data_pre
data_pre_mean = data_pre[['Problem Solving Skills', 'Creativity', 'Critical Thinking']].mean()
data_pre_median = data_pre[['Problem Solving Skills', 'Creativity', 'Critical Thinking']].median()
data_pre_mode = data_pre[['Problem Solving Skills', 'Creativity', 'Critical Thinking']].mode().iloc[0]

# Display the results for data_post
print("Data Post - Mean:")
print(data_post_mean)
print("\nData Post - Median:")
print(data_post_median)
print("\nData Post - Mode:")
print(data_post_mode)

# Display the results for data_pre
print("\nData Pre - Mean:")
print(data_pre_mean)
print("\nData Pre - Median:")
print(data_pre_median)
print("\nData Pre - Mode:")
print(data_pre_mode)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming data_pre and data_post DataFrames already exist and have numeric values

# Set Seaborn aesthetics
sns.set(style="whitegrid")

# List of skills/metrics to analyze
metrics = ['Problem Solving Skills', 'Creativity', 'Critical Thinking']

# Create frequency distribution plots for data_pre
plt.figure(figsize=(18, 6))
for i, metric in enumerate(metrics, 1):
    plt.subplot(1, 3, i)

    # Plot histogram with KDE
    sns.histplot(data_pre[metric], kde=True, color='skyblue', bins=20, edgecolor='black')

    # Calculate mean, median, and mode
    mean_value = data_pre[metric].mean()
    median_value = data_pre[metric].median()
    mode_value = data_pre[metric].mode()[0]  # Mode can have multiple values; take the first one

    # Add vertical lines for mean, median, and mode
    plt.axvline(mean_value, color='red', linestyle='--', label=f'Mean: {mean_value:.2f}')
    plt.axvline(median_value, color='green', linestyle='-', label=f'Median: {median_value:.2f}')
    plt.axvline(mode_value, color='blue', linestyle='-.', label=f'Mode: {mode_value:.2f}')

    # Labeling
    plt.xlabel(f'{metric} Score', fontsize=14)
    plt.ylabel('Frequency', fontsize=14)
    plt.title(f'Pre Intervention - {metric}', fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.gca().grid(False)  # Remove the grid

    # Add legend
    plt.legend(fontsize=12)

plt.tight_layout()
plt.savefig("pre-intervention_stats_with_mean_median_modeAUG.png", dpi=300, bbox_inches='tight')
plt.show()

# Create frequency distribution plots for data_post
plt.figure(figsize=(18, 6))
for i, metric in enumerate(metrics, 1):
    plt.subplot(1, 3, i)

    # Plot histogram with KDE
    sns.histplot(data_post[metric], kde=True, color='lightcoral', bins=20, edgecolor='black')

    # Calculate mean, median, and mode
    mean_value = data_post[metric].mean()
    median_value = data_post[metric].median()
    mode_value = data_post[metric].mode()[0]  # Mode can have multiple values; take the first one

    # Add vertical lines for mean, median, and mode
    plt.axvline(mean_value, color='red', linestyle='--', label=f'Mean: {mean_value:.2f}')
    plt.axvline(median_value, color='green', linestyle='-', label=f'Median: {median_value:.2f}')
    plt.axvline(mode_value, color='blue', linestyle='-.', label=f'Mode: {mode_value:.2f}')

    # Labeling
    plt.xlabel(f'{metric} Score', fontsize=14)
    plt.ylabel('Frequency', fontsize=14)
    plt.title(f'Post Intervention - {metric}', fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.gca().grid(False)  # Remove the grid

    # Add legend
    plt.legend(fontsize=12)

plt.tight_layout()
plt.savefig("post-intervention_stats_with_mean_median_modeAUg.png", dpi=300, bbox_inches='tight')
plt.show()

import os
import zipfile

# Define the folder you want to compress and the output ZIP file path
folder_path = '/content/'  # Replace with the path to your folder
output_zip_path = '/content/FinalData_Augmented.zip'  # The path for the compressed ZIP file

# Create a list to hold the paths of the files you want to compress
files_to_compress = []

# Walk through the folder to find all .csv and .png files
for root, dirs, files in os.walk(folder_path):
    for file in files:
        if file.endswith('.csv') or file.endswith('.png'):
            files_to_compress.append(os.path.join(root, file))

# Create a ZIP file containing only .csv and .png files
with zipfile.ZipFile(output_zip_path, 'w') as zipf:
    for file in files_to_compress:
        zipf.write(file, os.path.relpath(file, folder_path))  # Store files with relative path

print(f"Selected files from '{folder_path}' have been compressed into: {output_zip_path}")
from google.colab import files

# Trigger the download of the ZIP file
files.download(output_zip_path)