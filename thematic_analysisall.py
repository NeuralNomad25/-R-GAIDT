# -*- coding: utf-8 -*-
"""Thematic_analysisall.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VZ7J_q0s74MmFi3urD4-jazmB_JN8bwt

## Loading Orignal Dataset
"""

import pandas as pd

thematic_df=pd.read_csv("/content/Post-interventionOrigninalSample.csv")

"""# New section"""

thematic_df.head()

# Get the last three columns
last_three_cols = thematic_df.iloc[:, -3:]

# Drop NaN values
refined_df = last_three_cols.dropna()

# Store the refined DataFrame in a new CSV file
refined_df.to_csv("/content/thematic_data.csv", index=False)

refined_df.tail()

# Rename columns
refined_df = refined_df.rename(columns={
    refined_df.columns[0]: 'R-GAIDT-Critical Analysis_Solution_EvaluationPBL',
    refined_df.columns[1]: 'R-GAIDT-GenAI_Designthinking_Problemsolving',
    refined_df.columns[2]: 'Overall_GenAI_DesignThinking_RGAIDT_Learningexperience_SkillDevlopment_PBL'
})

# Store the refined DataFrame in a new CSV file
refined_df.to_csv("/content/NLPThematicR-GAIDT.csv", index=False)
refined_df.tail()

"""## Thematic Analysis on 'R-GAIDT-Critical Analysis_Solution_EvaluationPB', i.e., How has the integration of the Generative AI tool within the R-GAIDT framework affected your ability to critically analyze and evaluate the solutions you developed during the project?

## cleaning and preprocessing
"""

import pandas as pd
# Store the refined DataFrame in a new CSV file
refined_df= pd.read_csv("/content/NLPThematicR-GAIDT.csv")
refined_df.tail()

import nltk
# Resetting NLTK's data path to the default location
nltk.data.path = ['/usr/nltk_data', '/root/nltk_data']

# Ensure punkt, stopwords, and wordnet are downloaded
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

import spacy

# Load spacy's pre-trained English tokenizer
nlp = spacy.load("en_core_web_sm")

# Preprocessing function using spaCy
def preprocess_text_spacy(text):
    doc = nlp(text.lower())
    return " ".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])

# Apply preprocessing using spaCy
refined_df['cleaned_text'] = refined_df['R-GAIDT-Critical Analysis_Solution_EvaluationPBL'].apply(preprocess_text_spacy)

refined_df['cleaned_text']

import pandas as pd
import spacy

# Load spaCy's pre-trained English tokenizer
nlp = spacy.load("en_core_web_sm")

# Preprocessing function using spaCy
def preprocess_text_spacy(text):
    doc = nlp(text.lower())
    return " ".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])

# Apply preprocessing using spaCy and store the cleaned text in the same column
refined_df['R-GAIDT-Critical Analysis_Solution_EvaluationPBL'] = refined_df['R-GAIDT-Critical Analysis_Solution_EvaluationPBL'].apply(preprocess_text_spacy)

# Save the cleaned DataFrame to a CSV file
refined_df[['R-GAIDT-Critical Analysis_Solution_EvaluationPBL']].to_csv("/content/R-GAIDT-Critical Analysis_Solution_EvaluationPBL.csv", index=False)

""" # Topic Modelling We will use Latent Dirichlet Allocation (LDA), a popular topic modeling technique, to find underlying topics (themes) in the cleaned text"""

import pandas as pd
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Vectorize the cleaned text using TF-IDF
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
X = vectorizer.fit_transform(refined_df['R-GAIDT-Critical Analysis_Solution_EvaluationPBL'])

# Apply Latent Dirichlet Allocation (LDA) for topic modeling
lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(X)

# Helper function to capitalize GAN, AI, and other words
def capitalize_special(text):
    """Capitalize first letter, and ensure GAN and AI are fully capitalized."""
    if text.lower() == 'gan':
        return 'GAN'
    elif text.lower() == 'ai':
        return 'AI'
    else:
        return text.capitalize()

# Save topic terms
n_words = 10
terms = vectorizer.get_feature_names_out()

for index, topic in enumerate(lda.components_):
    print(f"Topic #{index + 1}:")
    print([capitalize_special(terms[i]) for i in topic.argsort()[-n_words:]])
    print()

# Visualizing and saving the top words for each topic with enhanced styling
def plot_and_save_top_words(model, feature_names, n_top_words):
    colors = plt.cm.tab20.colors  # Colormap for distinct colors
    hatch_style = '//'  # Uniform hatch style for bars

    for topic_idx, topic in enumerate(model.components_):
        plt.figure(figsize=(9, 6))
        words = [capitalize_special(feature_names[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]
        importances = topic[topic.argsort()[:-n_top_words - 1:-1]]
        bars = plt.barh(words, importances, color=[colors[i % len(colors)] for i in range(len(words))], hatch=hatch_style)
        for bar in bars:
            bar.set_edgecolor('black')  # Black edges for bars

        plt.title(f"Topic #{topic_idx + 1}", fontsize=10, weight='bold')
        plt.xlabel('Importance', fontsize=12)
        plt.ylabel('Words', fontsize=12)
        plt.grid(False)  # Remove gridlines
        plt.tight_layout()

        # Save each topic as a high-resolution image
        plt.savefig(f"/content/topic_{topic_idx + 1}_words.png", dpi=300)
        plt.show()

# Plot and save top words for all topics
plot_and_save_top_words(lda, terms, 10)

# Create and save a WordCloud for the most frequent words in the dataset
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(refined_df['cleaned_text']))
plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
#plt.title('WordCloud of Frequent Words', fontsize=16, weight='bold')
# Save the word cloud as a high-resolution image
plt.savefig('/content/wordcloud_frequent_words.png', dpi=300, bbox_inches='tight')
plt.show()

from wordcloud import WordCloud

# Generate the word cloud for the entire text
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(refined_df['cleaned_text']))

# Extract the words and their frequencies
word_freq = wordcloud.words_

# Print the words and their frequencies
for word, freq in word_freq.items():
    print(f"{word}: {freq}")

import os
import zipfile
from google.colab import files

# Specify the directory and the extensions you want to download
directory = '/content/'
extensions = ['.csv', '.png']  # Add more extensions as needed

# List all files in the directory
files_in_directory = os.listdir(directory)

# Filter files with the specified extensions
files_to_download = [file for file in files_in_directory if any(file.endswith(ext) for ext in extensions)]

# Define the name of the zip file
zip_filename = '/content/Thematicanalysis.zip'

# Create a zip file and add the matching files to it
with zipfile.ZipFile(zip_filename, 'w') as zipf:
    for file in files_to_download:
        file_path = os.path.join(directory, file)
        zipf.write(file_path, os.path.basename(file))  # Add file to zip

# Download the zip file
files.download(zip_filename)

"""## Step-by-Step Guide for Clustering and Further Analysis: Clustering with K-Means:

# use K-Means clustering with TF-IDF vectors to group similar responses into clusters, which can provide insights into the main themes.


"""

from sklearn.cluster import KMeans

# Apply KMeans clustering to the text data
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(X)

# Assign the cluster labels to the DataFrame
refined_df['cluster'] = kmeans.labels_

# Print the clustered topics
for i in range(5):  # Adjust this depending on the number of clusters
    print(f"Cluster {i}:")
    cluster_words = [terms[j] for j in kmeans.cluster_centers_.argsort()[:, -10:]]
    print(cluster_words)

"""## Sentiment Analysis"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob

# Function to get sentiment polarity from TextBlob
def get_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

# Apply the sentiment function to your column 'R-GAIDT-Critical Analysis_Solution_EvaluationPBL'
refined_df['sentiment'] = refined_df['R-GAIDT-Critical Analysis_Solution_EvaluationPBL'].apply(get_sentiment)

# 1. Histogram of Sentiment Polarity with High-Quality Resolution
plt.figure(figsize=(7, 5))
plt.hist(refined_df['sentiment'], bins=30, color='skyblue', edgecolor='black', hatch='//')
plt.xlabel('Sentiment Polarity', fontsize=10)
plt.ylabel('Frequency', fontsize=10)

# Remove gridlines
plt.grid(False)

# Add a legend with the desired label
plt.legend(['R-GAIDT\'s Impact on Critical Analysis&Solution Evaluation'], fontsize=8)

# Save the histogram as a high-resolution PNG image
plt.tight_layout()
plt.savefig('/content/sentiment_polarity_distributionR-GAIDT-Critical Analysis.png', dpi=300)

plt.show()

# 2. Bar Plot to show Positive, Negative, and Neutral Sentiments with Hatching
sentiment_labels = ['Negative', 'Neutral', 'Positive']
sentiment_counts = [
    len(refined_df[refined_df['sentiment'] < -0.05]),  # Negative sentiment
    len(refined_df[(refined_df['sentiment'] >= -0.05) & (refined_df['sentiment'] <= 0.05)]),  # Neutral sentiment
    len(refined_df[refined_df['sentiment'] > 0.05])   # Positive sentiment
]

plt.figure(figsize=(7, 5))
sns.barplot(x=sentiment_labels, y=sentiment_counts, palette='coolwarm')

# Add hatching to the bars
for bar in plt.gca().patches:
    bar.set_hatch('//')

plt.xlabel('Sentiment', fontsize=10)
plt.ylabel('Count', fontsize=10)

# Remove gridlines
plt.grid(False)

# Add a legend with the desired label
plt.legend(['R-GAIDT\'s Impact on Critical Analysis&Solution Evaluation'], fontsize=8)

# Save the bar plot as a high-resolution PNG image
plt.tight_layout()
plt.savefig('/content/sentiment_analysisR-GAIDT-Critical Analysis.png', dpi=300)

plt.show()

# 3. Boxplot to visualize the spread of sentiment polarity with High-Quality Resolution
plt.figure(figsize=(7, 5))
sns.boxplot(x=refined_df['sentiment'], color='lightgreen')

# Customize the boxplot appearance
plt.xlabel('Sentiment Polarity', fontsize=10)

# Remove gridlines
plt.grid(False)

# Add a legend with the desired label
plt.legend(['R-GAIDT\'s Impact on Critical Analysis&Solution Evaluation'], fontsize=8)

# Save the boxplot as a high-resolution PNG image
plt.tight_layout()
plt.savefig('/content/sentiment_polarity_R-GAIDT-Critical Analysis.png', dpi=300)

plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob

# Function to get sentiment polarity from TextBlob
def get_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

# Apply the sentiment function to your column 'R-GAIDT-Critical Analysis_Solution_EvaluationPBL'
refined_df['sentiment'] = refined_df['R-GAIDT-Critical Analysis_Solution_EvaluationPBL'].apply(get_sentiment)

# 1. Print sentiment data and statistics before plotting
print("First 5 rows of sentiment data:")
print(refined_df[['R-GAIDT-Critical Analysis_Solution_EvaluationPBL', 'sentiment']].head(20))

print("\nSummary statistics of sentiment polarity:")
print(refined_df['sentiment'].describe())

# Sentiment counts for Negative, Neutral, and Positive
sentiment_counts = {
    'Negative': len(refined_df[refined_df['sentiment'] < -0.05]),
    'Neutral': len(refined_df[(refined_df['sentiment'] >= -0.05) & (refined_df['sentiment'] <= 0.05)]),
    'Positive': len(refined_df[refined_df['sentiment'] > 0.05])
}
print("\nSentiment counts:")
print(sentiment_counts)

# Plot 1: Histogram of Sentiment Polarity
print("\nPlotting histogram of sentiment polarity...")
plt.figure(figsize=(7, 5))
plt.hist(refined_df['sentiment'], bins=30, color='skyblue', edgecolor='black', hatch='//')
plt.xlabel('Sentiment Polarity', fontsize=10)
plt.ylabel('Frequency', fontsize=10)
plt.grid(False)
plt.legend(['R-GAIDT\'s Impact on Critical Analysis & Solution Evaluation'], fontsize=8)
plt.tight_layout()
plt.savefig('/content/sentiment_polarity_distributionR-GAIDT-Critical_Analysis.png', dpi=300)
plt.show()

# Plot 2: Bar Chart of Sentiment Categories with Hatching
print("\nPlotting bar chart of sentiment categories...")
plt.figure(figsize=(7, 5))
bars = sns.barplot(x=list(sentiment_counts.keys()), y=list(sentiment_counts.values()), palette='coolwarm')

# Add hatching to bars
for bar in bars.patches:
    bar.set_hatch('//')

plt.xlabel('Sentiment', fontsize=10)
plt.ylabel('Count', fontsize=10)
plt.grid(False)
plt.legend(['R-GAIDT\'s Impact on Critical Analysis & Solution Evaluation'], fontsize=8)
plt.tight_layout()
plt.savefig('/content/sentiment_analysisR-GAIDT-Critical_Analysis.png', dpi=300)
plt.show()

# Plot 3: Boxplot of Sentiment Polarity
print("\nPlotting boxplot of sentiment polarity...")
plt.figure(figsize=(7, 5))
sns.boxplot(x=refined_df['sentiment'], color='lightgreen')
plt.xlabel('Sentiment Polarity', fontsize=10)
plt.grid(False)
plt.legend(['R-GAIDT\'s Impact on Critical Analysis & Solution Evaluation'], fontsize=8)
plt.tight_layout()
plt.savefig('/content/sentiment_polarity_R-GAIDT-Critical_Analysis.png', dpi=300)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob

# Function to get sentiment polarity from TextBlob
def get_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

# Apply the sentiment function to your column 'R-GAIDT-Critical Analysis_Solution_EvaluationPBL'
refined_df['sentiment'] = refined_df['R-GAIDT-Critical Analysis_Solution_EvaluationPBL'].apply(get_sentiment)

# Print statistics for boxplot data
print("\nBoxplot Statistics for Sentiment Polarity:")
print(f"Minimum Sentiment Polarity: {refined_df['sentiment'].min()}")
print(f"Maximum Sentiment Polarity: {refined_df['sentiment'].max()}")
print(f"Mean Sentiment Polarity: {refined_df['sentiment'].mean()}")
print(f"Median Sentiment Polarity: {refined_df['sentiment'].median()}")
print(f"Standard Deviation of Sentiment Polarity: {refined_df['sentiment'].std()}")

# Plot 3: Boxplot of Sentiment Polarity
print("\nPlotting boxplot of sentiment polarity...")
plt.figure(figsize=(7, 5))
sns.boxplot(x=refined_df['sentiment'], color='lightgreen')
plt.xlabel('Sentiment Polarity', fontsize=10)
plt.grid(False)
plt.legend(['R-GAIDT\'s Impact on Critical Analysis & Solution Evaluation'], fontsize=8)
plt.tight_layout()
plt.savefig('/content/sentiment_polarity_R-GAIDT-Critical_Analysis.png', dpi=300)
plt.show()

import os
import zipfile
from google.colab import files

# Specify the directory and the extensions you want to download
directory = '/content/'
extensions = ['.csv', '.png']  # Add more extensions as needed

# List all files in the directory
files_in_directory = os.listdir(directory)

# Filter files with the specified extensions
files_to_download = [file for file in files_in_directory if any(file.endswith(ext) for ext in extensions)]

# Define the name of the zip file
zip_filename = '/content/sentianalysis.zip'

# Create a zip file and add the matching files to it
with zipfile.ZipFile(zip_filename, 'w') as zipf:
    for file in files_to_download:
        file_path = os.path.join(directory, file)
        zipf.write(file_path, os.path.basename(file))  # Add file to zip

# Download the zip file
files.download(zip_filename)

"""## Reflect on how the use of Generative AI tools and the design thinking framework provided by R-GAIDT contributed to your problem-solving skills."""

import pandas as pd
import matplotlib.pyplot as plt
df=pd.read_csv("/content/NLPThematicR-GAIDT.csv")
df.head()
refined_df=df.copy()
refined_df.head()

import nltk
# Resetting NLTK's data path to the default location
nltk.data.path = ['/usr/nltk_data', '/root/nltk_data']

# Ensure punkt, stopwords, and wordnet are downloaded
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

import spacy

# Load spacy's pre-trained English tokenizer
nlp = spacy.load("en_core_web_sm")

# Preprocessing function using spaCy
def preprocess_text_spacy(text):
    doc = nlp(text.lower())
    return " ".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])

# Apply preprocessing using spaCy
refined_df['R-GAIDT-GenAI_Designthinking_Problemsolving'] = refined_df['R-GAIDT-GenAI_Designthinking_Problemsolving'].apply(preprocess_text_spacy)

refined_df.head()

"""## Topic Modelling"""

import pandas as pd
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Vectorize the cleaned text using TF-IDF
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
X = vectorizer.fit_transform(refined_df['R-GAIDT-GenAI_Designthinking_Problemsolving'])

# Apply Latent Dirichlet Allocation (LDA) for topic modeling
lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(X)

# Helper function to capitalize GAN, AI, and other words
def capitalize_special(text):
    """Capitalize first letter, and ensure GAN and AI are fully capitalized."""
    if text.lower() == 'gan':
        return 'GAN'
    elif text.lower() == 'ai':
        return 'AI'
    else:
        return text.capitalize()

# Save topic terms
n_words = 10
terms = vectorizer.get_feature_names_out()

for index, topic in enumerate(lda.components_[1:6], start=1):
    print(f"Topic #{index }:")
    print([capitalize_special(terms[i]) for i in topic.argsort()[-n_words:]])
    print()

# Visualizing and saving the top words for each topic with enhanced styling
def plot_and_save_top_words(model, feature_names, n_top_words):
    colors = plt.cm.tab20.colors  # Colormap for distinct colors
    hatch_style = '//'  # Uniform hatch style for bars

    for topic_idx, topic in enumerate(model.components_[1:6], start=1):
        plt.figure(figsize=(9, 6))
        words = [capitalize_special(feature_names[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]
        importances = topic[topic.argsort()[:-n_top_words - 1:-1]]
        bars = plt.barh(words, importances, color=[colors[i % len(colors)] for i in range(len(words))], hatch=hatch_style)
        for bar in bars:
            bar.set_edgecolor('black')  # Black edges for bars

        plt.title(f"Topic #{topic_idx}", fontsize=10, weight='bold')
        plt.xlabel('Importance', fontsize=12)
        plt.ylabel('Words', fontsize=12)
        plt.grid(False)  # Remove gridlines
        plt.tight_layout()

        # Save each topic as a high-resolution image
        plt.savefig(f"/content/topic_{topic_idx + 1}_problemsolving.png", dpi=300)
        plt.show()

# Plot and save top words for all topics
plot_and_save_top_words(lda, terms, 10)

# Create and save a WordCloud for the most frequent words in the dataset
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(refined_df['cleaned_text']))
plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
#plt.title('WordCloud of Frequent Words', fontsize=16, weight='bold')
# Save the word cloud as a high-resolution image
plt.savefig('/content/wordcloud_problemsolving.png', dpi=300, bbox_inches='tight')
plt.show()

"""## sentiment analysis for problem solving"""

from wordcloud import WordCloud

# Generate the word cloud for the entire text
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(refined_df['cleaned_text']))

# Extract the words and their frequencies
word_freq = wordcloud.words_

# Print the words and their frequencies
for word, freq in word_freq.items():
    print(f"{word}: {freq}")

"""##Reflect on how the use of Generative AI tools and the design thinking framework provided by R-GAIDT contributed to your problem-solving skills. 'Impact of R-GAIDT'GenAI & Design Thinking on Problem Solving'"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob

# Function to get sentiment polarity from TextBlob
def get_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

# Apply the sentiment function to your column 'R-GAIDT-Critical Analysis_Solution_EvaluationPBL'
refined_df['sentiment'] = refined_df['R-GAIDT-GenAI_Designthinking_Problemsolving'].apply(get_sentiment)

# 1. Print sentiment data and statistics before plotting
print("First 5 rows of sentiment data:")
print(refined_df[['R-GAIDT-GenAI_Designthinking_Problemsolving', 'sentiment']].head(20))

print("\nSummary statistics of sentiment polarity:")
print(refined_df['sentiment'].describe())

# Sentiment counts for Negative, Neutral, and Positive
sentiment_counts = {
    'Negative': len(refined_df[refined_df['sentiment'] < -0.05]),
    'Neutral': len(refined_df[(refined_df['sentiment'] >= -0.05) & (refined_df['sentiment'] <= 0.05)]),
    'Positive': len(refined_df[refined_df['sentiment'] > 0.05])
}
print("\nSentiment counts:")
print(sentiment_counts)

# Plot 1: Histogram of Sentiment Polarity
print("\nPlotting histogram of sentiment polarity...")
plt.figure(figsize=(7, 5))
plt.hist(refined_df['sentiment'], bins=30, color='skyblue', edgecolor='black', hatch='//')
plt.xlabel('Sentiment Polarity', fontsize=10)
plt.ylabel('Frequency', fontsize=10)
plt.grid(False)
plt.legend(['Impact of R-GAIDT\'s GenAI & Design Thinking on Problem Solving'], fontsize=8)
plt.tight_layout()
plt.savefig('/content/sentiment_polarity_distrtibution_problemsolving.png', dpi=300)
plt.show()

# Plot 2: Bar Chart of Sentiment Categories with Hatching
print("\nPlotting bar chart of sentiment categories...")
plt.figure(figsize=(7, 5))
bars = sns.barplot(x=list(sentiment_counts.keys()), y=list(sentiment_counts.values()), palette='coolwarm')

# Add hatching to bars
for bar in bars.patches:
    bar.set_hatch('//')

plt.xlabel('Sentiment', fontsize=10)
plt.ylabel('Count', fontsize=10)
plt.grid(False)
plt.legend(['Impact of R-GAIDT\'s GenAI & Design Thinking on Problem Solving'], fontsize=8)
plt.tight_layout()
plt.savefig('/content/sentiment_analysisR-GAIDT-_problemsolving.png', dpi=300)
plt.show()

# Plot 3: Boxplot of Sentiment Polarity
print("\nPlotting boxplot of sentiment polarity...")
plt.figure(figsize=(7, 5))
sns.boxplot(x=refined_df['sentiment'], color='lightgreen')
plt.xlabel('Sentiment Polarity', fontsize=10)
plt.grid(False)
plt.legend(['Impact of R-GAIDT\'s GenAI & Design Thinking on Problem Solving'], fontsize=8)
plt.tight_layout()
plt.savefig('/content/sentiment_polarity_R-GAIDT-_problemsolving.png', dpi=300)
plt.show()

import os
import zipfile
from google.colab import files

# Specify the directory and the extensions you want to download
directory = '/content/'
extensions = ['.csv', '.png']  # Add more extensions as needed

# List all files in the directory
files_in_directory = os.listdir(directory)

# Filter files with the specified extensions
files_to_download = [file for file in files_in_directory if any(file.endswith(ext) for ext in extensions)]

# Define the name of the zip file
zip_filename = '/content/sentianalysisProblemsolvingGenAIDT.zip'

# Create a zip file and add the matching files to it
with zipfile.ZipFile(zip_filename, 'w') as zipf:
    for file in files_to_download:
        file_path = os.path.join(directory, file)
        zipf.write(file_path, os.path.basename(file))  # Add file to zip

# Download the zip file
files.download(zip_filename)

"""## 3. Overall, how did the combination of design thinking and the Generative AI tool within the R-GAIDT framework enhance your learning experience and skill development throughout the project?"""

import pandas as pd
import matplotlib.pyplot as plt
df=pd.read_csv("/content/NLPThematicR-GAIDT.csv")
df.head()
refined_df=df.copy()
refined_df.head()

import nltk
# Resetting NLTK's data path to the default location
nltk.data.path = ['/usr/nltk_data', '/root/nltk_data']

# Ensure punkt, stopwords, and wordnet are downloaded
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

import spacy

# Load spacy's pre-trained English tokenizer
nlp = spacy.load("en_core_web_sm")

# Preprocessing function using spaCy
def preprocess_text_spacy(text):
    doc = nlp(text.lower())
    return " ".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])

# Apply preprocessing using spaCy
refined_df['Overall_GenAI_DesignThinking_RGAIDT_Learningexperience_SkillDevlopment_PBL'] = refined_df['Overall_GenAI_DesignThinking_RGAIDT_Learningexperience_SkillDevlopment_PBL'].apply(preprocess_text_spacy)

refined_df.head()

"""## Topic Modelling"""

import pandas as pd
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Vectorize the cleaned text using TF-IDF
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
X = vectorizer.fit_transform(refined_df['Overall_GenAI_DesignThinking_RGAIDT_Learningexperience_SkillDevlopment_PBL'])

# Apply Latent Dirichlet Allocation (LDA) for topic modeling
lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(X)

# Helper function to capitalize GAN, AI, and other words
def capitalize_special(text):
    """Capitalize first letter, and ensure GAN and AI are fully capitalized."""
    if text.lower() == 'gan':
        return 'GAN'
    elif text.lower() == 'ai':
        return 'AI'
    else:
        return text.capitalize()

# Save topic terms
n_words = 10
terms = vectorizer.get_feature_names_out()

for index, topic in enumerate(lda.components_[1:6], start=1):
    print(f"Topic #{index }:")
    print([capitalize_special(terms[i]) for i in topic.argsort()[-n_words:]])
    print()

# Visualizing and saving the top words for each topic with enhanced styling
def plot_and_save_top_words(model, feature_names, n_top_words):
    colors = plt.cm.tab20.colors  # Colormap for distinct colors
    hatch_style = '//'  # Uniform hatch style for bars

    for topic_idx, topic in enumerate(model.components_[1:6], start=1):
        plt.figure(figsize=(9, 6))
        words = [capitalize_special(feature_names[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]
        importances = topic[topic.argsort()[:-n_top_words - 1:-1]]
        bars = plt.barh(words, importances, color=[colors[i % len(colors)] for i in range(len(words))], hatch=hatch_style)
        for bar in bars:
            bar.set_edgecolor('black')  # Black edges for bars

        plt.title(f"Topic #{topic_idx}", fontsize=10, weight='bold')
        plt.xlabel('Importance', fontsize=12)
        plt.ylabel('Words', fontsize=12)
        plt.grid(False)  # Remove gridlines
        plt.tight_layout()

        # Save each topic as a high-resolution image
        plt.savefig(f"/content/topic_{topic_idx + 1}_overalllearning_skills.png", dpi=300)
        plt.show()

# Plot and save top words for all topics
plot_and_save_top_words(lda, terms, 10)

# Create and save a WordCloud for the most frequent words in the dataset
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(refined_df['Overall_GenAI_DesignThinking_RGAIDT_Learningexperience_SkillDevlopment_PBL']))
plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
#plt.title('WordCloud of Frequent Words', fontsize=16, weight='bold')
# Save the word cloud as a high-resolution image
plt.savefig('/content/wordcloud_overalllearning_skills.png', dpi=300, bbox_inches='tight')
plt.show()

"""## sentiment analysis for problem solving"""

from wordcloud import WordCloud

# Generate the word cloud for the entire text
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(refined_df['Overall_GenAI_DesignThinking_RGAIDT_Learningexperience_SkillDevlopment_PBL']))

# Extract the words and their frequencies
word_freq = wordcloud.words_

# Print the words and their frequencies
for word, freq in word_freq.items():
    print(f"{word}: {freq}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob

# Function to get sentiment polarity from TextBlob
def get_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

# Apply the sentiment function to your column 'R-GAIDT-Critical Analysis_Solution_EvaluationPBL'
refined_df['sentiment'] = refined_df['Overall_GenAI_DesignThinking_RGAIDT_Learningexperience_SkillDevlopment_PBL'].apply(get_sentiment)

# 1. Print sentiment data and statistics before plotting
print("First 5 rows of sentiment data:")
print(refined_df[['Overall_GenAI_DesignThinking_RGAIDT_Learningexperience_SkillDevlopment_PBL', 'sentiment']].head(20))

print("\nSummary statistics of sentiment polarity:")
print(refined_df['sentiment'].describe())

# Sentiment counts for Negative, Neutral, and Positive
sentiment_counts = {
    'Negative': len(refined_df[refined_df['sentiment'] < -0.05]),
    'Neutral': len(refined_df[(refined_df['sentiment'] >= -0.05) & (refined_df['sentiment'] <= 0.05)]),
    'Positive': len(refined_df[refined_df['sentiment'] > 0.05])
}
print("\nSentiment counts:")
print(sentiment_counts)

# Plot 1: Histogram of Sentiment Polarity
print("\nPlotting histogram of sentiment polarity...")
plt.figure(figsize=(7, 5))
plt.hist(refined_df['sentiment'], bins=30, color='skyblue', edgecolor='black', hatch='//')
plt.xlabel('Sentiment Polarity', fontsize=10)
plt.ylabel('Frequency', fontsize=10)
plt.grid(False)
plt.legend(['Impact of R-GAIDT\'s GenAI & Design Thinking on Overall Learning & Skills'], fontsize=8)
plt.tight_layout()
plt.savefig('/content/sentiment_polarity_learning_skills.png', dpi=300)
plt.show()

# Plot 2: Bar Chart of Sentiment Categories with Hatching
print("\nPlotting bar chart of sentiment categories...")
plt.figure(figsize=(7, 5))
bars = sns.barplot(x=list(sentiment_counts.keys()), y=list(sentiment_counts.values()), palette='coolwarm')

# Add hatching to bars
for bar in bars.patches:
    bar.set_hatch('//')

plt.xlabel('Sentiment', fontsize=10)
plt.ylabel('Count', fontsize=10)
plt.grid(False)
plt.legend(['Impact of R-GAIDT\'s GenAI & Design Thinking on Overall Learning & Skills'], fontsize=8)
plt.tight_layout()
plt.savefig('/content/sentiment_analysisR-GAIDT_learning_skills.png', dpi=300)
plt.show()

# Plot 3: Boxplot of Sentiment Polarity
print("\nPlotting boxplot of sentiment polarity...")
plt.figure(figsize=(7, 5))
sns.boxplot(x=refined_df['sentiment'], color='lightgreen')
plt.xlabel('Sentiment Polarity', fontsize=10)
plt.grid(False)
plt.legend(['Impact of R-GAIDT\'s GenAI & Design Thinking on Overall Learning & Skills'], fontsize=8)
plt.tight_layout()
plt.savefig('/content/sentiment_polarity_R-GAIDT_learning_skills.png', dpi=300)
plt.show()

import os
import zipfile
from google.colab import files

# Specify the directory and the extensions you want to download
directory = '/content/'
extensions = ['.csv', '.png']  # Add more extensions as needed

# List all files in the directory
files_in_directory = os.listdir(directory)

# Filter files with the specified extensions
files_to_download = [file for file in files_in_directory if any(file.endswith(ext) for ext in extensions)]

# Define the name of the zip file
zip_filename = '/content/sentianalysisOverallLearningskillsGenAIDT.zip'

# Create a zip file and add the matching files to it
with zipfile.ZipFile(zip_filename, 'w') as zipf:
    for file in files_to_download:
        file_path = os.path.join(directory, file)
        zipf.write(file_path, os.path.basename(file))  # Add file to zip

# Download the zip file
files.download(zip_filename)